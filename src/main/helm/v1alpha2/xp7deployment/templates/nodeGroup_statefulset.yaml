# Create StatefulSet for each nodeGroup
{{- range $nodeGroup := .Values.deployment.spec.nodeGroups }}
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ $nodeGroup.name }}
  namespace: {{ $.Values.deployment.namespace }}
  annotations:
    {{ $.Values.annotationKeys.applyPriority }}: "90"
  labels:
    {{ $.Values.labelKeys.managed }}: true
    {{ $.Values.labelKeys.nodeGroup }}: {{ $nodeGroup.name }}
    {{- with $.Values.defaultLabels }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
  ownerReferences:
    {{- with $.Values.ownerReferences }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
spec:
  serviceName: {{ $.Values.discoveryService }}
  replicas: {{ if $.Values.deployment.spec.enabled }}{{ $nodeGroup.replicas }}{{ else }}0{{end}}
  revisionHistoryLimit: 10
  podManagementPolicy: Parallel
  updateStrategy:
    type: {{ $.Values.pods.updateStrategy }}
  selector:
    matchLabels:
      {{ $.Values.labelKeys.managed }}: true
      {{ $.Values.labelKeys.nodeGroup }}: {{ $nodeGroup.name }}
      {{ $.Values.labelKeys.master }}: {{ $nodeGroup.master }}
      {{ $.Values.labelKeys.data }}: {{ $nodeGroup.data }}
      {{- with $.Values.defaultLabels }}
      {{- toYaml . | nindent 6 }}
      {{- end }}
  template:
    metadata:
      annotations:
        enonic.cloud/metrics.xp7: "true"
        enonic.cloud/logs.xp7: "true"
        backup.velero.io/backup-volumes: "{{- join "," $.Values.volumes.backups }}"
        pre.hook.backup.velero.io/container: "exp"
        pre.hook.backup.velero.io/command: '["bash", "-c", "/usr/local/bin/backup.sh"]'
        {{- if index $.Values "settings.linkerd" }}
        linkerd.io/inject: enabled
        {{- end }}
      labels:
        {{ $.Values.labelKeys.managed }}: true
        {{ $.Values.labelKeys.nodeGroup }}: {{ $nodeGroup.name }}
        {{ $.Values.labelKeys.master }}: {{ $nodeGroup.master }}
        {{ $.Values.labelKeys.data }}: {{ $nodeGroup.data }}
        {{- with $.Values.defaultLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
    spec:
      serviceAccount: xp-node-sa
      serviceAccountName: xp-node-sa
      {{- if $.Values.pods.nodeGroupAntiAffinity }}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: {{ $.Values.labelKeys.nodeGroup }}
                    operator: In
                    values:
                      - {{ $nodeGroup.name }}
              topologyKey: "kubernetes.io/hostname"
      {{- end }}
      restartPolicy: Always
      dnsPolicy: ClusterFirst
      schedulerName: default-scheduler
      terminationGracePeriodSeconds: {{ $.Values.pods.terminationGracePeriodSeconds }}

      securityContext:
        fsGroup: {{ $.Values.pods.securityContext.fsGroup }}

      containers:
        - name: exp
          image: {{ $.Values.image.nameTemplate | replace "%s" $.Values.deployment.spec.xpVersion }}
          imagePullPolicy: {{ $.Values.image.pullPolicy }}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File

          securityContext:
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            runAsUser: {{ $.Values.pods.securityContext.user }}
            sysctls: # TODO: Is this creating unnecessary updates?
              - name: vm.max_map_count
                value: "262144"

          resources:
            requests:
              cpu: {{ if $.Values.resources.cpu.defaultRequests }}{{ $.Values.resources.cpu.defaultRequests }}{{ else }}{{ $nodeGroup.resources.cpu }}{{ end }}
              memory: {{ if $.Values.resources.memory.defaultRequests }}{{ $.Values.resources.memory.defaultRequests }}{{ else }}{{ $nodeGroup.resources.memory }}{{ end }}
            limits:
              cpu: {{ $nodeGroup.resources.cpu }}
              memory: {{ $nodeGroup.resources.memory }}

          ports:
            - name: xp-stats
              containerPort: 2609
              protocol: TCP
            - name: xp-main
              containerPort: 8080
              protocol: TCP
            - name: es-http
              containerPort: 9200
              protocol: TCP
            - name: es-transport
              containerPort: 9300
              protocol: TCP

          env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: XP_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: XP_NODE_GROUP
              value: {{ $nodeGroup.name }}
            - name: XP_CLUSTERED # This is to ensure that XP is restarted when deployment is changed from clustered to non clustered
              value: {{ if $.Values.deployment.clustered }}true{{ else }}false{{ end }}
            - name: XP_NODE_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: SU_PASS_HASH
              valueFrom:
                secretKeyRef:
                  name: su
                  key: passHash
                  optional: false
          {{- range $env := $nodeGroup.env }}
            - name: {{ $env.name }}
              value: {{ $env.value }}
          {{- end }}

          readinessProbe:
            failureThreshold: 2
            httpGet:
              {{- if and $nodeGroup.data $.Values.deployment.clustered }}
              path: /_cluster/health?wait_for_status=green&timeout=1s
              {{ else }}
              path: /_cluster/health?wait_for_status=yellow&timeout=1s
              {{- end }}
              port: 9200
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 5
            successThreshold: 3
            timeoutSeconds: 5

          {{- if $.Values.deployment.clustered }}
          lifecycle:
            preStop:
              exec:
                command: ["/bin/bash","{{ $.Values.dirs.extraConfig }}/preStop.sh"]
          {{- end }}

          volumeMounts:
            - name: config
              mountPath: {{ $.Values.dirs.config }}
            - name: extra-config
              mountPath: {{ $.Values.dirs.extraConfig }}
            - name: extra-config
              mountPath: {{ $.Values.files.setEnv }}
              subPath: preStart.sh
            {{- range $volume := $nodeGroup.resources.disks }}
            - name: {{ $volume.name }}
              mountPath: {{ index $.Values.volumes.mounts $volume.name }}
            {{- end }}
            {{- range $volume := $.Values.deployment.spec.nodesSharedDisks }}
            - name: {{ $volume.name }}
              mountPath: {{ index $.Values.volumes.mounts $volume.name }}
            {{- end }}
      volumes:
        - name: config
          configMap:
            defaultMode: "420"
            name: {{ $nodeGroup.name }}
        - name: extra-config
          configMap:
            defaultMode: "420"
            name: extra-config
        {{- range $volume := $.Values.deployment.spec.nodesSharedDisks }}
        - name: {{ $volume.name }}
          persistentVolumeClaim:
            claimName: {{ $volume.name }}
        {{- end }}

  volumeClaimTemplates:
    {{- range $volume := $nodeGroup.resources.disks }}
    - metadata:
        name: {{ $volume.name }}
        annotations:
          {{- with $.Values.storage.default.annotations }}
          {{- toYaml . | nindent 10 }}
          {{- end }}
        labels:
          {{- if not (has $volume.name $.Values.volumes.backups) }}
          velero.io/exclude-from-backup: true
          {{- end }}
          {{ $.Values.labelKeys.managed }}: true
          {{ $.Values.labelKeys.nodeGroup }}: {{ $nodeGroup.name }}
          {{- with $.Values.defaultLabels }}
          {{- toYaml . | nindent 10 }}
          {{- end }}
      spec:
        volumeMode: Filesystem
        storageClassName: {{ $.Values.storage.default.storageClassName }}
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: {{ $volume.size }}
    {{- end }}
---
{{- end }}